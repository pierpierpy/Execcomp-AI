{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02576249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, Image, ClassLabel\n",
    "\n",
    "# Config\n",
    "SEED = 42424242\n",
    "SAMPLE_SIZE_NEGATIVE = 3000\n",
    "BASE_PATH = Path(\"/home/pdipasquale/MIIA/stuff\")\n",
    "OUTPUT_PATH = BASE_PATH / \"output\"\n",
    "ALL_TABLES_PATH = BASE_PATH / \"all_tables.json\"\n",
    "HF_REPO = \"pierjoe/sec-table-classifier\"\n",
    "\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Categorize documents\n",
    "sct_docs, non_sct_docs, funds, multi_sct_docs = [], [], [], []\n",
    "\n",
    "for doc_dir in tqdm(list(OUTPUT_PATH.iterdir()), desc=\"Scanning docs\"):\n",
    "    if not doc_dir.is_dir(): continue\n",
    "    metadata_path = doc_dir / \"metadata.json\"\n",
    "    if not metadata_path.exists(): continue\n",
    "    \n",
    "    with open(metadata_path) as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    if meta.get(\"sic\") in (\"NULL\", None):\n",
    "        funds.append(doc_dir.name)\n",
    "        continue\n",
    "    \n",
    "    classification_path = doc_dir / \"classification_results.json\"\n",
    "    if classification_path.exists():\n",
    "        with open(classification_path) as f:\n",
    "            classification = json.load(f)\n",
    "        num_sct = classification.get(\"total_tables_found\", 0)\n",
    "        if num_sct == 1:\n",
    "            sct_docs.append({\"doc_id\": doc_dir.name, \"meta\": meta, \"classification\": classification})\n",
    "        else:\n",
    "            multi_sct_docs.append(doc_dir.name)\n",
    "    elif (doc_dir / \"no_sct_found.json\").exists():\n",
    "        non_sct_docs.append(doc_dir.name)\n",
    "\n",
    "print(f\"Funds: {len(funds)} | SCT (1 table): {len(sct_docs)} | SCT (multi): {len(multi_sct_docs)} | No SCT: {len(non_sct_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbff21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build positive samples (SCT tables)\n",
    "positive_samples = []\n",
    "for doc in tqdm(sct_docs, desc=\"Positive samples\"):\n",
    "    doc_id = doc[\"doc_id\"]\n",
    "    for table_entry in doc[\"classification\"].get(\"tables\", []):\n",
    "        table_data = table_entry[\"table\"]\n",
    "        img_path = OUTPUT_PATH / doc_id / doc_id / \"vlm\" / table_data.get(\"img_path\", \"\")\n",
    "        table_body = table_data.get(\"table_body\", \"\")\n",
    "        if img_path.exists() and table_body:\n",
    "            positive_samples.append({\n",
    "                \"doc_id\": doc_id, \"image_path\": str(img_path), \"table_html\": table_body,\n",
    "                \"label\": 1, \"year\": doc[\"meta\"].get(\"year\"), \"company\": doc[\"meta\"].get(\"company\")\n",
    "            })\n",
    "print(f\"Positive samples: {len(positive_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load all tables and build negative samples\n",
    "with open(ALL_TABLES_PATH) as f:\n",
    "    all_tables = json.load(f)\n",
    "\n",
    "# Build SCT table keys to exclude\n",
    "sct_table_keys = {(s[\"doc_id\"], \"/\".join(s[\"image_path\"].split(\"/\")[-2:])) for s in positive_samples}\n",
    "sct_doc_ids = {doc[\"doc_id\"] for doc in sct_docs}\n",
    "\n",
    "# Get negative samples - tables from SCT docs that are NOT the SCT table\n",
    "all_negative_candidates = []\n",
    "for table in tqdm(all_tables, desc=\"Negative candidates\"):\n",
    "    doc_id = table.get(\"source_doc\")\n",
    "    if doc_id not in sct_doc_ids: continue\n",
    "    \n",
    "    img_rel_path = table.get(\"img_path\", \"\")\n",
    "    if (doc_id, img_rel_path) in sct_table_keys: continue\n",
    "    \n",
    "    img_path = OUTPUT_PATH / doc_id / doc_id / \"vlm\" / img_rel_path\n",
    "    table_body = table.get(\"table_body\", \"\")\n",
    "    if not img_path.exists() or not table_body: continue\n",
    "    \n",
    "    meta_path = OUTPUT_PATH / doc_id / \"metadata.json\"\n",
    "    meta = json.load(open(meta_path)) if meta_path.exists() else {}\n",
    "    \n",
    "    all_negative_candidates.append({\n",
    "        \"doc_id\": doc_id, \"image_path\": str(img_path), \"table_html\": table_body,\n",
    "        \"label\": 0, \"year\": meta.get(\"year\"), \"company\": meta.get(\"company\")\n",
    "    })\n",
    "\n",
    "negative_samples = random.sample(all_negative_candidates, min(SAMPLE_SIZE_NEGATIVE, len(all_negative_candidates)))\n",
    "print(f\"Negative samples: {len(negative_samples)} (from {len(all_negative_candidates)} candidates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create HuggingFace dataset\n",
    "all_samples = positive_samples + negative_samples\n",
    "random.shuffle(all_samples)\n",
    "print(f\"Total: {len(all_samples)} | Positive: {len(positive_samples)} | Negative: {len(negative_samples)}\")\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"image\": [s[\"image_path\"] for s in all_samples],\n",
    "    \"text\": [s[\"table_html\"] for s in all_samples],\n",
    "    \"label\": [s[\"label\"] for s in all_samples],\n",
    "    \"doc_id\": [s[\"doc_id\"] for s in all_samples],\n",
    "    \"year\": [s[\"year\"] for s in all_samples],\n",
    "    \"company\": [s[\"company\"] for s in all_samples],\n",
    "})\n",
    "dataset = dataset.cast_column(\"image\", Image())\n",
    "dataset = dataset.cast_column(\"label\", ClassLabel(names=[\"non_sct\", \"sct\"]))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split and push to HuggingFace\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=SEED, stratify_by_column=\"label\")\n",
    "print(f\"Train: {len(dataset_split['train'])} | Test: {len(dataset_split['test'])}\")\n",
    "\n",
    "dataset_split.push_to_hub(HF_REPO, private=False)\n",
    "print(f\"âœ“ Pushed to: https://huggingface.co/datasets/{HF_REPO}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
