{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02576249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, Image, ClassLabel\n",
    "\n",
    "# Config\n",
    "SEED = 42424242\n",
    "SAMPLE_SIZE_NEGATIVE = 10_000\n",
    "BASE_PATH = Path(\"/home/pdipasquale/MIIA/stuff\")\n",
    "OUTPUT_PATH = BASE_PATH / \"output\"\n",
    "ALL_TABLES_PATH = BASE_PATH / \"all_tables.json\"\n",
    "HF_REPO = \"pierjoe/sec-table-classifier\"\n",
    "\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654a2759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning docs: 100%|██████████| 9558/9558 [00:06<00:00, 1440.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funds: 1717 | SCT (1 table): 5009 | SCT (multi): 1123 | No SCT: 431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Categorize documents\n",
    "sct_docs, non_sct_docs, funds, multi_sct_docs = [], [], [], []\n",
    "\n",
    "for doc_dir in tqdm(list(OUTPUT_PATH.iterdir()), desc=\"Scanning docs\"):\n",
    "    if not doc_dir.is_dir(): continue\n",
    "    metadata_path = doc_dir / \"metadata.json\"\n",
    "    if not metadata_path.exists(): continue\n",
    "    \n",
    "    with open(metadata_path) as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    if meta.get(\"sic\") in (\"NULL\", None):\n",
    "        funds.append(doc_dir.name)\n",
    "        continue\n",
    "    \n",
    "    classification_path = doc_dir / \"classification_results.json\"\n",
    "    if classification_path.exists():\n",
    "        with open(classification_path) as f:\n",
    "            classification = json.load(f)\n",
    "        num_sct = classification.get(\"total_tables_found\", 0)\n",
    "        if num_sct == 1:\n",
    "            sct_docs.append({\"doc_id\": doc_dir.name, \"meta\": meta, \"classification\": classification})\n",
    "        else:\n",
    "            multi_sct_docs.append(doc_dir.name)\n",
    "    elif (doc_dir / \"no_sct_found.json\").exists():\n",
    "        non_sct_docs.append(doc_dir.name)\n",
    "\n",
    "print(f\"Funds: {len(funds)} | SCT (1 table): {len(sct_docs)} | SCT (multi): {len(multi_sct_docs)} | No SCT: {len(non_sct_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bbff21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positive samples: 100%|██████████| 5009/5009 [00:09<00:00, 554.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 4998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build positive samples (SCT tables)\n",
    "positive_samples = []\n",
    "for doc in tqdm(sct_docs, desc=\"Positive samples\"):\n",
    "    doc_id = doc[\"doc_id\"]\n",
    "    for table_entry in doc[\"classification\"].get(\"tables\", []):\n",
    "        table_data = table_entry[\"table\"]\n",
    "        img_path = OUTPUT_PATH / doc_id / doc_id / \"vlm\" / table_data.get(\"img_path\", \"\")\n",
    "        table_body = table_data.get(\"table_body\", \"\")\n",
    "        if img_path.exists() and table_body:\n",
    "            positive_samples.append({\n",
    "                \"doc_id\": doc_id, \"image_path\": str(img_path), \"table_html\": table_body,\n",
    "                \"label\": 1, \"year\": doc[\"meta\"].get(\"year\"), \"company\": doc[\"meta\"].get(\"company\")\n",
    "            })\n",
    "print(f\"Positive samples: {len(positive_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd2a8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Negative candidates: 100%|██████████| 165797/165797 [01:07<00:00, 2466.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples: 10000 (from 82583 candidates)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load all tables and build negative samples\n",
    "with open(ALL_TABLES_PATH) as f:\n",
    "    all_tables = json.load(f)\n",
    "\n",
    "# Build SCT table keys to exclude\n",
    "sct_table_keys = {(s[\"doc_id\"], \"/\".join(s[\"image_path\"].split(\"/\")[-2:])) for s in positive_samples}\n",
    "sct_doc_ids = {doc[\"doc_id\"] for doc in sct_docs}\n",
    "\n",
    "# Get negative samples - tables from SCT docs that are NOT the SCT table\n",
    "all_negative_candidates = []\n",
    "for table in tqdm(all_tables, desc=\"Negative candidates\"):\n",
    "    doc_id = table.get(\"source_doc\")\n",
    "    if doc_id not in sct_doc_ids: continue\n",
    "    \n",
    "    img_rel_path = table.get(\"img_path\", \"\")\n",
    "    if (doc_id, img_rel_path) in sct_table_keys: continue\n",
    "    \n",
    "    img_path = OUTPUT_PATH / doc_id / doc_id / \"vlm\" / img_rel_path\n",
    "    table_body = table.get(\"table_body\", \"\")\n",
    "    if not img_path.exists() or not table_body: continue\n",
    "    \n",
    "    meta_path = OUTPUT_PATH / doc_id / \"metadata.json\"\n",
    "    meta = json.load(open(meta_path)) if meta_path.exists() else {}\n",
    "    \n",
    "    all_negative_candidates.append({\n",
    "        \"doc_id\": doc_id, \"image_path\": str(img_path), \"table_html\": table_body,\n",
    "        \"label\": 0, \"year\": meta.get(\"year\"), \"company\": meta.get(\"company\")\n",
    "    })\n",
    "\n",
    "negative_samples = random.sample(all_negative_candidates, min(SAMPLE_SIZE_NEGATIVE, len(all_negative_candidates)))\n",
    "print(f\"Negative samples: {len(negative_samples)} (from {len(all_negative_candidates)} candidates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8832f362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 14998 | Positive: 4998 | Negative: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 14998/14998 [00:00<00:00, 750491.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text', 'label', 'doc_id', 'year', 'company'],\n",
      "    num_rows: 14998\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create HuggingFace dataset\n",
    "all_samples = positive_samples + negative_samples\n",
    "random.shuffle(all_samples)\n",
    "print(f\"Total: {len(all_samples)} | Positive: {len(positive_samples)} | Negative: {len(negative_samples)}\")\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"image\": [s[\"image_path\"] for s in all_samples],\n",
    "    \"text\": [s[\"table_html\"] for s in all_samples],\n",
    "    \"label\": [s[\"label\"] for s in all_samples],\n",
    "    \"doc_id\": [s[\"doc_id\"] for s in all_samples],\n",
    "    \"year\": [s[\"year\"] for s in all_samples],\n",
    "    \"company\": [s[\"company\"] for s in all_samples],\n",
    "})\n",
    "dataset = dataset.cast_column(\"image\", Image())\n",
    "dataset = dataset.cast_column(\"label\", ClassLabel(names=[\"non_sct\", \"sct\"]))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b5ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 11998 | Test: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:02<00:00, 1515.91 examples/s]ards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00,  8.83ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  331MB /  331MB, 47.3MB/s  \n",
      "New Data Upload: 100%|██████████|  326MB /  326MB, 46.6MB/s  \n",
      "Map: 100%|██████████| 3999/3999 [00:02<00:00, 1680.12 examples/s]13.79s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00,  8.71ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  336MB /  336MB, 76.1MB/s  \n",
      "New Data Upload: 100%|██████████|  331MB /  331MB, 75.3MB/s  \n",
      "Map: 100%|██████████| 3999/3999 [00:02<00:00, 1891.43 examples/s]12.73s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00,  8.35ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  336MB /  336MB, 76.2MB/s  \n",
      "New Data Upload: 100%|██████████|  332MB /  332MB, 75.5MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 3/3 [00:45<00:00, 15.22s/ shards]\n",
      "Map: 100%|██████████| 3000/3000 [00:01<00:00, 1931.14 examples/s]ards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  8.59ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  252MB /  252MB, 60.0MB/s  \n",
      "New Data Upload: 100%|██████████|  249MB /  249MB, 59.4MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.10s/ shards]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pushed to: https://huggingface.co/datasets/pierjoe/sec-table-classifier\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Split and push to HuggingFace\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=SEED, stratify_by_column=\"label\")\n",
    "print(f\"Train: {len(dataset_split['train'])} | Test: {len(dataset_split['test'])}\")\n",
    "\n",
    "dataset_split.push_to_hub(HF_REPO, private=False)\n",
    "print(f\"✓ Pushed to: https://huggingface.co/datasets/{HF_REPO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6de9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
