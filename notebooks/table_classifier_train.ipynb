{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa48e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Table Classifier Training Dataset Builder\n",
    "Creates a dataset for SCT vs non-SCT table classification.\n",
    "\"\"\"\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, Image, ClassLabel\n",
    "\n",
    "# Config\n",
    "SEED = 42424242\n",
    "SAMPLE_SIZE_NEGATIVE = 3000\n",
    "BASE_PATH = Path(\"/home/pdipasquale/MIIA/stuff\")\n",
    "OUTPUT_PATH = BASE_PATH / \"output\"\n",
    "ALL_TABLES_PATH = BASE_PATH / \"all_tables.json\"\n",
    "HF_REPO = \"pierjoe/sec-table-classifier\"\n",
    "\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "426f968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning docs: 100%|██████████| 2500/2500 [00:01<00:00, 1750.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funds: 463 | SCT (1 table): 1549 | SCT (multi): 363 | No SCT: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Categorize documents\n",
    "sct_docs, non_sct_docs, funds, multi_sct_docs = [], [], [], []\n",
    "\n",
    "for doc_dir in tqdm(list(OUTPUT_PATH.iterdir()), desc=\"Scanning docs\"):\n",
    "    if not doc_dir.is_dir(): continue\n",
    "    metadata_path = doc_dir / \"metadata.json\"\n",
    "    if not metadata_path.exists(): continue\n",
    "    \n",
    "    with open(metadata_path) as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    if meta.get(\"sic\") in (\"NULL\", None):\n",
    "        funds.append(doc_dir.name)\n",
    "        continue\n",
    "    \n",
    "    classification_path = doc_dir / \"classification_results.json\"\n",
    "    if classification_path.exists():\n",
    "        with open(classification_path) as f:\n",
    "            classification = json.load(f)\n",
    "        num_sct = classification.get(\"total_tables_found\", 0)\n",
    "        if num_sct == 1:\n",
    "            sct_docs.append({\"doc_id\": doc_dir.name, \"meta\": meta, \"classification\": classification})\n",
    "        else:\n",
    "            multi_sct_docs.append(doc_dir.name)\n",
    "    elif (doc_dir / \"no_sct_found.json\").exists():\n",
    "        non_sct_docs.append(doc_dir.name)\n",
    "\n",
    "print(f\"Funds: {len(funds)} | SCT (1 table): {len(sct_docs)} | SCT (multi): {len(multi_sct_docs)} | No SCT: {len(non_sct_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7400337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positive samples: 100%|██████████| 1549/1549 [00:01<00:00, 907.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 1549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build positive samples (SCT tables)\n",
    "positive_samples = []\n",
    "for doc in tqdm(sct_docs, desc=\"Positive samples\"):\n",
    "    doc_id = doc[\"doc_id\"]\n",
    "    for table_entry in doc[\"classification\"].get(\"tables\", []):\n",
    "        table_data = table_entry[\"table\"]\n",
    "        img_path = OUTPUT_PATH / doc_id / doc_id / \"vlm\" / table_data.get(\"img_path\", \"\")\n",
    "        table_body = table_data.get(\"table_body\", \"\")\n",
    "        if img_path.exists() and table_body:\n",
    "            positive_samples.append({\n",
    "                \"doc_id\": doc_id, \"image_path\": str(img_path), \"table_html\": table_body,\n",
    "                \"label\": 1, \"year\": doc[\"meta\"].get(\"year\"), \"company\": doc[\"meta\"].get(\"company\")\n",
    "            })\n",
    "print(f\"Positive samples: {len(positive_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e890930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Negative candidates: 100%|██████████| 52564/52564 [00:15<00:00, 3474.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples: 3000 (from 25414 candidates)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load all tables and build negative samples\n",
    "with open(ALL_TABLES_PATH) as f:\n",
    "    all_tables = json.load(f)\n",
    "\n",
    "# Build SCT table keys to exclude\n",
    "sct_table_keys = {(s[\"doc_id\"], \"/\".join(s[\"image_path\"].split(\"/\")[-2:])) for s in positive_samples}\n",
    "sct_doc_ids = {doc[\"doc_id\"] for doc in sct_docs}\n",
    "\n",
    "# Get negative samples - tables from SCT docs that are NOT the SCT table\n",
    "all_negative_candidates = []\n",
    "for table in tqdm(all_tables, desc=\"Negative candidates\"):\n",
    "    doc_id = table.get(\"source_doc\")\n",
    "    if doc_id not in sct_doc_ids: continue\n",
    "    \n",
    "    img_rel_path = table.get(\"img_path\", \"\")\n",
    "    if (doc_id, img_rel_path) in sct_table_keys: continue\n",
    "    \n",
    "    img_path = OUTPUT_PATH / doc_id / doc_id / \"vlm\" / img_rel_path\n",
    "    table_body = table.get(\"table_body\", \"\")\n",
    "    if not img_path.exists() or not table_body: continue\n",
    "    \n",
    "    meta_path = OUTPUT_PATH / doc_id / \"metadata.json\"\n",
    "    meta = json.load(open(meta_path)) if meta_path.exists() else {}\n",
    "    \n",
    "    all_negative_candidates.append({\n",
    "        \"doc_id\": doc_id, \"image_path\": str(img_path), \"table_html\": table_body,\n",
    "        \"label\": 0, \"year\": meta.get(\"year\"), \"company\": meta.get(\"company\")\n",
    "    })\n",
    "\n",
    "negative_samples = random.sample(all_negative_candidates, min(SAMPLE_SIZE_NEGATIVE, len(all_negative_candidates)))\n",
    "print(f\"Negative samples: {len(negative_samples)} (from {len(all_negative_candidates)} candidates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c990ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 4549 | Positive: 1549 | Negative: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 4549/4549 [00:00<00:00, 958066.23 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text', 'label', 'doc_id', 'year', 'company'],\n",
      "    num_rows: 4549\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create HuggingFace dataset\n",
    "all_samples = positive_samples + negative_samples\n",
    "random.shuffle(all_samples)\n",
    "print(f\"Total: {len(all_samples)} | Positive: {len(positive_samples)} | Negative: {len(negative_samples)}\")\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"image\": [s[\"image_path\"] for s in all_samples],\n",
    "    \"text\": [s[\"table_html\"] for s in all_samples],\n",
    "    \"label\": [s[\"label\"] for s in all_samples],\n",
    "    \"doc_id\": [s[\"doc_id\"] for s in all_samples],\n",
    "    \"year\": [s[\"year\"] for s in all_samples],\n",
    "    \"company\": [s[\"company\"] for s in all_samples],\n",
    "})\n",
    "dataset = dataset.cast_column(\"image\", Image())\n",
    "dataset = dataset.cast_column(\"label\", ClassLabel(names=[\"non_sct\", \"sct\"]))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7316a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3639 | Test: 910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3639/3639 [00:02<00:00, 1675.62 examples/s]ards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00,  9.06ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  303MB /  303MB, 50.4MB/s  \n",
      "New Data Upload: 100%|██████████|  298MB /  298MB, 49.7MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:12<00:00, 12.31s/ shards]\n",
      "Map: 100%|██████████| 910/910 [00:00<00:00, 1933.73 examples/s]shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  7.86ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 77.6MB / 77.6MB, 19.1MB/s  \n",
      "New Data Upload: 100%|██████████| 76.3MB / 76.3MB, 19.1MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.15s/ shards]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pushed to: https://huggingface.co/datasets/pierjoe/sec-table-classifier\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Split and push to HuggingFace\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=SEED, stratify_by_column=\"label\")\n",
    "print(f\"Train: {len(dataset_split['train'])} | Test: {len(dataset_split['test'])}\")\n",
    "\n",
    "dataset_split.push_to_hub(HF_REPO, private=False)\n",
    "print(f\"✓ Pushed to: https://huggingface.co/datasets/{HF_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047833b",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7054c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image as PILImage\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LR = 1e-5\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "985b985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen/Qwen2.5-VL-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Load base model and processor\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "print(f\"Model loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eb02ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 1,050,114\n"
     ]
    }
   ],
   "source": [
    "# Classifier model with classification head on top of VLM\n",
    "class VLMClassifier(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size=None, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        # Get hidden size from config\n",
    "        hidden_size = hidden_size or base_model.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, pixel_values, image_grid_thw, labels=None):\n",
    "        # Get model outputs (last hidden state)\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            image_grid_thw=image_grid_thw,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        # Use last hidden state, take mean over sequence\n",
    "        hidden_states = outputs.hidden_states[-1]  # (batch, seq, hidden)\n",
    "        pooled = hidden_states.mean(dim=1)  # (batch, hidden)\n",
    "        logits = self.classifier(pooled.float())  # (batch, num_labels)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# Freeze base model, only train classifier head\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = VLMClassifier(base_model, num_labels=2).to(DEVICE)\n",
    "print(f\"Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18c1ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1820 | Test batches: 455\n"
     ]
    }
   ],
   "source": [
    "# Custom collate function for VLM\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in batch:\n",
    "        img = item[\"image\"]\n",
    "        if isinstance(img, dict) and \"path\" in img:\n",
    "            img = PILImage.open(img[\"path\"]).convert(\"RGB\")\n",
    "        images.append(img)\n",
    "        labels.append(item[\"label\"])\n",
    "    \n",
    "    # Create simple prompt for classification\n",
    "    messages_batch = []\n",
    "    for img in images:\n",
    "        messages_batch.append([{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": \"Classify this table.\"}\n",
    "            ]\n",
    "        }])\n",
    "    \n",
    "    # Process with Qwen processor\n",
    "    texts = [processor.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages_batch]\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return inputs\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(dataset_split[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset_split[\"test\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "print(f\"Train batches: {len(train_loader)} | Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b599286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   3%|▎         | 55/1820 [00:12<06:56,  4.24it/s, acc=64.55%, loss=0.4473]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        # Move to device\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE, dtype=torch.bfloat16)\n",
    "        image_grid_thw = batch[\"image_grid_thw\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, pixel_values, image_grid_thw, labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs[\"logits\"].argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{correct/total:.2%}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f} | Acc: {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE, dtype=torch.bfloat16)\n",
    "        image_grid_thw = batch[\"image_grid_thw\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, pixel_values, image_grid_thw)\n",
    "        preds = outputs[\"logits\"].argmax(dim=-1)\n",
    "        \n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "print(f\"\\nTest Accuracy: {correct/total:.2%}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from collections import Counter\n",
    "print(f\"Predictions: {Counter(all_preds)}\")\n",
    "print(f\"Labels: {Counter(all_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the classifier head\n",
    "SAVE_PATH = BASE_PATH / \"models\" / \"sct_classifier\"\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save(model.classifier.state_dict(), SAVE_PATH / \"classifier_head.pt\")\n",
    "print(f\"✓ Classifier saved to: {SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
